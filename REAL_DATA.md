# Using Real Ecommerce Data

This guide shows you how to replace synthetic data with real ecommerce datasets for better model training.

## ðŸŽ¯ Why Real Data?

| Synthetic Data | Real Data |
|---------------|-----------|
| âœ… Quick to start | âœ… Authentic language patterns |
| âœ… No cleanup needed | âœ… Real product names/prices |
| âœ… Balanced dataset | âœ… Actual customer behavior |
| âŒ Not domain-specific | âœ… Better generalization |
| âŒ Less realistic | âŒ Requires data cleaning |

## ðŸ“Š Available Real Datasets

### 1. **Amazon Product Dataset** (Recommended)
- **Size**: 5GB
- **Products**: 2M+
- **Reviews**: 200M+
- **Includes**: Titles, descriptions, prices, brands, categories
- **Best For**: Product description generation

### 2. **eCommerce Transactions**
- **Size**: 50MB
- **Transactions**: 500K+
- **Includes**: Customer purchases, invoices, product codes
- **Best For**: Recommendations, customer behavior

### 3. **RetailRocket Catalog**
- **Size**: 1.5GB
- **Products**: 120K
- **Events**: Clicks, add-to-cart, transactions
- **Best For**: Product categorization, recommendations

### 4. **Google Reviews**
- **Size**: 2GB
- **Reviews**: 10M+
- **Includes**: Review text, ratings, business info
- **Best For**: Sentiment analysis, review summarization

## ðŸš€ Quick Start (3 Steps)

### Step 1: Download Datasets (10 minutes)

```bash
# Install Kaggle CLI
pip install kaggle

# Setup Kaggle (one-time)
# 1. Go to https://kaggle.com/account
# 2. Create API token â†’ Download kaggle.json
# 3. Run:
mkdir -p ~/.kaggle
cp kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Download all datasets (~8GB total)
python scripts/download_datasets.py --dataset all
```

**Output:**
```
ðŸ“¦ DOWNLOADING ECOMMERCE DATASETS
============================================================
âœ“ Kaggle CLI installed

ðŸ“¥ Downloading amazon_products...
   Description: Amazon product metadata...
   Expected size: ~5000MB
   Output directory: data/raw/amazon
âœ“ amazon_products downloaded successfully

âœ“ amazon_products: 4500.5MB
âœ“ ecommerce_transactions: 48.2MB
âœ“ retailrocket: 1450.3MB
âœ“ google_reviews: 1890.7MB

âœ… Download Summary: 4/4 datasets downloaded
```

### Step 2: Process & Combine (5 minutes)

```bash
# Process all datasets into training format
python scripts/prepare_real_data.py
```

**Output:**
```
ðŸ”§ PREPARING REAL ECOMMERCE DATASETS
============================================================

ðŸ“¦ Processing Amazon products...
   Loaded 2000000 products
   Processed 15000 products...
   âœ“ Created 15000 product description samples

ðŸ›’ Processing eCommerce transactions...
   Loaded 500000 transactions
   âœ“ Created 5000 recommendation samples

â­ Processing reviews...
   Loaded 10000000 reviews
   âœ“ Created 10000 review analysis samples

ðŸ’¬ Adding conversational commerce data...
   âœ“ Created 1000 conversational samples

ðŸ”„ Combining datasets...
   Total samples: 31000
   Training samples: 24800
   Evaluation samples: 6200
   âœ“ Saved to data/processed/ecommerce_train_real.json
   âœ“ Saved to data/processed/ecommerce_eval_real.json
```

### Step 3: Train Model (3 hours)

```bash
# Train with real data
python scripts/train.py \
    --model_name meta-llama/Llama-3.1-8B-Instruct \
    --output_dir outputs/elm-real-data
```

**That's it! Your model is trained on real ecommerce data!** ðŸŽ‰

## ðŸ“‚ Data Structure

After processing, you'll have:

```
data/
â”œâ”€â”€ raw/                          # Raw downloaded datasets
â”‚   â”œâ”€â”€ amazon/
â”‚   â”‚   â”œâ”€â”€ products.csv          # Amazon product metadata
â”‚   â”‚   â””â”€â”€ reviews.csv           # Amazon reviews
â”‚   â”œâ”€â”€ transactions/
â”‚   â”‚   â””â”€â”€ data.csv             # Transaction history
â”‚   â”œâ”€â”€ retailrocket/
â”‚   â”‚   â””â”€â”€ category.csv         # Product catalog
â”‚   â””â”€â”€ reviews/
â”‚       â””â”€â”€ data.csv             # Google reviews
â”‚
â”œâ”€â”€ processed/                     # Training-ready data
â”‚   â”œâ”€â”€ ecommerce_train_real.json   # 24,800 samples (80%)
â”‚   â”œâ”€â”€ ecommerce_eval_real.json    # 6,200 samples (20%)
â”‚   â””â”€â”€ dataset_summary.json       # Metadata
â”‚
â””â”€â”€ synthetic/                    # Original synthetic data
    â”œâ”€â”€ ecommerce_train.json
    â””â”€â”€ ecommerce_eval.json
```

## ðŸŽ¨ Training Data Breakdown

### Real Data (31,000 samples)

```
Product Descriptions    15,000 (48%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Conversational         10,000 (32%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Recommendations        3,000 (10%)  â–ˆâ–ˆâ–ˆâ–ˆ
Sentiment Analysis     3,000 (10%)  â–ˆâ–ˆâ–ˆâ–ˆ
```

### Combined Data (35,000 samples - Recommended)

```
Real Data         31,000 (89%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Synthetic Data     4,000 (11%)  â–ˆâ–ˆâ–ˆâ–ˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total             35,000 samples
```

**Why combine?** Real data provides authenticity, synthetic data ensures complete coverage.

## âš™ï¸ Advanced Usage

### Process Specific Dataset Only

```bash
# Only Amazon products
python scripts/prepare_real_data.py --dataset amazon

# Only transactions
python scripts/prepare_real_data.py --dataset transactions

# Skip conversations (use only real data)
python scripts/prepare_real_data.py --skip-conversations
```

### Download Individual Datasets

```bash
# Just Amazon products
python scripts/download_datasets.py --dataset amazon_products

# Just transactions
python scripts/download_datasets.py --dataset ecommerce_transactions
```

### Verify Downloads

```bash
# Check if datasets were downloaded correctly
python scripts/download_datasets.py --verify
```

## ðŸ”§ Custom Data Sources

### Use Your Own Shopify Store

```python
import requests
import pandas as pd

# Get products from Shopify API
store_url = "https://yourstore.myshopify.com"
api_url = f"{store_url}/admin/api/2024-01/products.json"
headers = {"X-Shopify-Access-Token": "YOUR_TOKEN"}

response = requests.get(api_url, headers=headers)
products = response.json()["products"]

# Convert to CSV
df = pd.DataFrame(products)
df.to_csv("data/raw/my_store_products.csv", index=False)

# Then process it
python scripts/prepare_custom_data.py --input data/raw/my_store_products.csv
```

### Use Customer Service Logs

```python
import json

# Process chat logs
with open('data/raw/chat_logs.jsonl', 'r') as f:
    chats = [json.loads(line) for line in f]

# Convert to training format
training_data = []
for chat in chats:
    data_point = {
        "instruction": "You are a customer service assistant.",
        "input": f"Customer: {chat['customer_message']}",
        "output": chat['agent_response']
    }
    training_data.append(data_point)

# Save
with open('data/processed/my_conversations.json', 'w') as f:
    for item in training_data:
        json.dump(item, f)
        f.write('\n')
```

## ðŸ“Š Data Quality Checks

### Check Dataset Quality

```python
import json
import pandas as pd

# Load training data
with open('data/processed/ecommerce_train_real.json', 'r') as f:
    data = [json.loads(line) for line in f]

print(f"Total samples: {len(data)}")

# Check for missing fields
missing = sum(1 for item in data if not item.get('instruction'))
print(f"Missing instructions: {missing}")

# Check average length
avg_input_len = sum(len(item['input']) for item in data) / len(data)
print(f"Average input length: {avg_input_len:.0f} chars")

# Check for duplicates
unique = len(set(item['input'] for item in data))
print(f"Unique inputs: {unique}/{len(data)}")
```

### Fix Common Issues

```python
# Remove duplicates
seen = set()
unique_data = []
for item in data:
    key = item['input'][:100]  # First 100 chars
    if key not in seen:
        seen.add(key)
        unique_data.append(item)

# Clean text
import re
for item in unique_data:
    item['input'] = re.sub(r'\s+', ' ', item['input']).strip()
    item['output'] = re.sub(r'\s+', ' ', item['output']).strip()

# Save cleaned data
with open('data/processed/ecommerce_train_clean.json', 'w') as f:
    for item in unique_data:
        json.dump(item, f)
        f.write('\n')
```

## ðŸ’¡ Best Practices

### 1. **Start Small**
```bash
# Test with 1,000 samples first
head -1000 data/processed/ecommerce_train_real.json > data/processed/ecommerce_train_small.json
```

### 2. **Mix Real + Synthetic**
```python
# Combine real and synthetic data
import json

# Load real data
with open('data/processed/ecommerce_train_real.json', 'r') as f:
    real_data = [json.loads(line) for line in f]

# Load synthetic data
with open('data/processed/ecommerce_train.json', 'r') as f:
    synth_data = [json.loads(line) for line in f]

# Combine (70% real, 30% synthetic)
combined = real_data + synth_data[:int(0.3 * len(real_data))]

# Save
with open('data/processed/ecommerce_train_combined.json', 'w') as f:
    for item in combined:
        json.dump(item, f)
        f.write('\n')
```

### 3. **Clean Data**
```python
# Remove HTML tags, fix encoding, etc.
import re
from bs4 import BeautifulSoup

def clean_text(text):
    # Remove HTML
    text = BeautifulSoup(text, 'html.parser').get_text()
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

for item in data:
    item['input'] = clean_text(item['input'])
    item['output'] = clean_text(item['output'])
```

### 4. **Domain-Specific**
```bash
# Filter to specific category (e.g., Electronics)
grep -i "electronics" data/processed/ecommerce_train_real.json > data/processed/electronics_train.json
```

## ðŸš¨ Troubleshooting

### Download Fails

```bash
# Check Kaggle authentication
kaggle datasets list

# If auth fails:
# 1. Go to https://kaggle.com/account
# 2. Regenerate API token
# 3. Update ~/.kaggle/kaggle.json
```

### Out of Memory During Processing

```python
# Process in chunks
chunk_size = 10000
for i in range(0, len(df), chunk_size):
    chunk = df[i:i+chunk_size]
    # Process chunk
```

### Dataset Not Found

```bash
# Use alternative sources
cat data/raw/ALTERNATIVE_SOURCES.md

# Or use synthetic data
python scripts/prepare_data.py
```

## ðŸ“ˆ Performance Comparison

| Metric | Synthetic Data | Real Data | Combined |
|--------|----------------|-----------|----------|
| Realism | 6/10 | 9/10 | 9/10 |
| Coverage | 8/10 | 7/10 | 9/10 |
| Training Speed | Fast | Medium | Medium |
| Model Quality | 7/10 | 8/10 | 9/10 |
| Ease of Use | 10/10 | 6/10 | 7/10 |

**Recommendation**: Start with synthetic â†’ Switch to real â†’ Combine for best results

## ðŸŽ‰ Summary

âœ… **Download**: Real datasets from Kaggle (8GB)
âœ… **Process**: Convert to training format (30K samples)
âœ… **Train**: Use in fine-tuning pipeline
âœ… **Result**: More realistic and accurate model

**Ready to start?** Run:
```bash
python scripts/download_datasets.py --dataset all
```

---
**Next**: See [DATASETS.md](DATASETS.md) for full dataset list
